{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495dba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# from natsort import natsorted\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import csv\n",
    "import json\n",
    "from pandas.core.frame import DataFrame\n",
    "import jsonlines\n",
    "import re\n",
    "import torch.optim as optim\n",
    "from random import *\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974995c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    " \n",
    "def un_gz(file_name):\n",
    "    \n",
    "    # 获取文件的名称，去掉后缀名\n",
    "    f_name = file_name.replace(\".gz\", \"\")\n",
    "    # 开始解压\n",
    "    g_file = gzip.GzipFile(file_name)\n",
    "    #读取解压后的文件，并写入去掉后缀名的同名文件（即得到解压后的文件）\n",
    "    open(f_name, \"wb+\").write(g_file.read())\n",
    "    g_file.close()\n",
    "    \n",
    "# un_gz('/Users/pupu/Desktop/毕设/java/java/final/jsonl/test/java_test_0.jsonl.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79693ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz('/Users/pupu/Desktop/毕设/java/java/final/jsonl/train/java_train_0.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04725c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location='/Users/pupu/Desktop/毕设/java/java/final/jsonl/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ae83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b39d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_1.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_2.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc09cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_3.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_4.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf3612",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_5.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_6.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ba772",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_7.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25601791",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_8.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5332520",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_9.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_10.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df426b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_11.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac44489",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_12.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_13.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bf6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_14.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c98889",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz(train_location+'/java_train_15.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a96fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(train_location+'/java_train_1.jsonl','r') as load_f:\n",
    "#         load_dict = json.load(load_f)\n",
    "#         print(load_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca72574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_trans(file):\n",
    "    with open(file,'r+', encoding='utf-8') as f:\n",
    "        list_repo=[]\n",
    "        list_path=[]\n",
    "        list_func_name=[]\n",
    "        list_original_string=[]\n",
    "        list_language=[]\n",
    "        list_code=[]\n",
    "        list_code_tokens=[]\n",
    "        list_docstring=[]\n",
    "        list_docstring_tokens=[]\n",
    "        list_sha=[]\n",
    "        list_url=[]\n",
    "        list_partition=[]\n",
    "        for item in jsonlines.Reader(f): # 每一行读取后都是一个json，可以按照key去取对应的值\n",
    "\n",
    "            repo = item['repo']\n",
    "            list_repo.append(repo)\n",
    "\n",
    "            path = item['path']\n",
    "            list_path.append(path)\n",
    "\n",
    "            func_name = item['func_name']\n",
    "            list_func_name.append(func_name)\n",
    "\n",
    "            original_string = item['original_string']\n",
    "            list_original_string.append(original_string)\n",
    "\n",
    "            language = item['language']\n",
    "            list_language.append(language)\n",
    "\n",
    "            code = item['code']\n",
    "            list_code.append(code)\n",
    "\n",
    "            code_tokens = item['code_tokens']\n",
    "            list_code_tokens.append(code_tokens)\n",
    "\n",
    "            docstring = item['docstring']\n",
    "            list_docstring.append(docstring)\n",
    "\n",
    "            docstring_tokens = item['docstring_tokens']\n",
    "            list_docstring_tokens.append(docstring_tokens)\n",
    "\n",
    "            sha = item['sha']\n",
    "            list_sha.append(sha)\n",
    "\n",
    "            url = item['url']\n",
    "            list_url.append(url)\n",
    "\n",
    "            partition = item['partition']\n",
    "            list_partition.append(partition)\n",
    "\n",
    "        data_train_1={'repo':list_repo,\"path\":list_path,\"func_name\":list_func_name,\n",
    "                      \"original_string\":list_original_string,\"language\":list_language,\"code\":list_code,\n",
    "                      \"code_tokens\":list_code_tokens,\"docstring\":list_docstring,\n",
    "                      \"docstring_tokens\":list_docstring_tokens,\"sha\":list_sha,\"url\":list_url,\n",
    "                     \"partition\":list_partition}\n",
    "        df_train_1=pd.DataFrame(data_train_1)\n",
    "\n",
    "    return df_train_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284576d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=file_trans(train_location+'/java_train_0.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=file_trans(train_location+'/java_train_1.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b96ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=file_trans(train_location+'/java_train_2.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80296386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=file_trans(train_location+'/java_train_3.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=file_trans(train_location+'/java_train_4.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32405368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=file_trans(train_location+'/java_train_5.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ab69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=file_trans(train_location+'/java_train_6.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7=file_trans(train_location+'/java_train_7.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d493641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8=file_trans(train_location+'/java_train_8.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9=file_trans(train_location+'/java_train_9.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10=file_trans(train_location+'/java_train_10.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11=file_trans(train_location+'/java_train_11.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12=file_trans(train_location+'/java_train_12.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df13=file_trans(train_location+'/java_train_13.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d768bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df14=file_trans(train_location+'/java_train_14.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df15=file_trans(train_location+'/java_train_15.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f731522",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train=pd.concat([df0,df1, df2, df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=file_trans('/Users/pupu/Desktop/毕设/java/java/final/jsonl/test/java_test_0.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_gz('/Users/pupu/Desktop/毕设/java/java/final/jsonl/valid/java_valid_0.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid=file_trans('/Users/pupu/Desktop/毕设/java/java/final/jsonl/valid/java_valid_0.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7102fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_train['docstring_clean']=''\n",
    "for i,j in df_train.iterrows():\n",
    "    caption = j['docstring']\n",
    "    caption.replace('\\n',' ')\n",
    "    str_list = caption.split(\" \")\n",
    "    list2=[]\n",
    "    for f in str_list:\n",
    "        if not f.isalpha():\n",
    "            str_list.remove(f)\n",
    "        if not f.islower():\n",
    "            f = f.lower()\n",
    "            \n",
    "        list2.append(f)\n",
    "    tab = ' '\n",
    "    list1 = tab.join(list2)\n",
    "    list1 = re.sub(r'[^\\w\\s]','',list1)\n",
    "    df_train.loc[i,'docstring_clean'] =list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29903320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,j in df_train.iterrows():\n",
    " \n",
    "    caption = j['docstring_clean'].replace('\\n', '').replace('\\r', '')\n",
    "    j['docstring_clean'] = caption\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02fa8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbb892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('/Users/pupu/Desktop/毕设/java/java/final/jsonl/train/df_train.csv')#将文件保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.to_csv('/Users/pupu/Desktop/毕设/java/java/final/jsonl/valid/df_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('/Users/pupu/Desktop/毕设/java/java/final/jsonl/test/df_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c325065",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**<font color = blue size=5 face=雅黑>前面是数据表处理 处理完的csv已经存在本地 从这里读取</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('/Users/pupu/Desktop/毕设/java/java/final/jsonl/train/df_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2217ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1=df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d75553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1 = df_train1[0:20000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4783da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train1['docstring_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28cb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid=pd.read_csv('/Users/pupu/Desktop/毕设/java/java/final/jsonl/valid/df_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('/Users/pupu/Desktop/毕设/java/java/final/jsonl/test/df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    def __init__(self):\n",
    "        # intially, set both the IDs and words to dictionaries with special tokens\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1, '<end>': 2}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<end>'}\n",
    "        self.idx = 3\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # if the word does not already exist in the dictionary, add it\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            # increment the ID for the next word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # if we try to access a word not in the dictionary, return the id for <unk>\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca09606",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i ,row in df_train1.iterrows():\n",
    "        ch=str(row['docstring_clean'])\n",
    "        linee=re.sub('[\\u4e00 - \\u9fff]', '', ch)\n",
    "        if len(ch.encode('utf8')) != len(ch):\n",
    "            df_train1=df_train1.drop(index=i)\n",
    "         #去中文\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce78d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae8398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8e126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_caption = []\n",
    "for i,row in df_train1.iterrows():\n",
    "    a = row['docstring_clean']\n",
    "    clean_caption.append(a)\n",
    "    #clean_caption.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72323608",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = []\n",
    "for i in clean_caption:\n",
    "    j = str(i).split()\n",
    "    for i in j:\n",
    "        clean_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(clean_list)\n",
    "word = [x[0] for x in count.items() if x[1] > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary()\n",
    "for i in word:\n",
    "    vocabulary.add_word(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c893e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcfc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "word=list(set(clean_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78701655",
   "metadata": {},
   "outputs": [],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d38ee2",
   "metadata": {},
   "source": [
    "**<font color = blue size=5 face=雅黑>转为token</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
    "for i, w in enumerate(word):\n",
    "    word2idx[w] = i + 4\n",
    "idx2word = {i: w for i, w in enumerate(word2idx)}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in clean_caption:\n",
    "    arr = [word2idx[s] for s in str(sentence).split()]\n",
    "    token_list.append(arr)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d3ebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbb618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b0e2e9c",
   "metadata": {},
   "source": [
    "**<font color = blue size=5 face=雅黑>转为tensor</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample IsNext and NotNext to be same in small batch size\n",
    "def make_data():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index = randrange(len(clean_caption)), randrange(len(clean_caption)) # sample random index in sentences\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM\n",
    "        n_pred =  min(max_pred, max(1, int(len(input_ids) * 0.15))) # 15 % of tokens in one sentence\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] # candidate masked position\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word2idx['[MASK]'] # make mask\n",
    "            elif random() > 0.9:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                while index < 4: # can't involve 'CLS', 'SEP', 'PAD'\n",
    "                  index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = index # replace\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "    return batch\n",
    "# Proprecessing Finished\n",
    "\n",
    "batch = make_data()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
    "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "      def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.isNext = isNext\n",
    "\n",
    "      def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "      def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af2bda",
   "metadata": {},
   "source": [
    "**<font color = blue size=5 face=雅黑>Model\n",
    "</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 130\n",
    "batch_size = 6\n",
    "max_pred = 5 # max tokens of prediction\n",
    "n_layers = 6\n",
    "n_heads = 12\n",
    "d_model = 768\n",
    "d_ff = 768*4 # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc59f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, seq_len = seq_q.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "    return pad_attn_mask.expand(batch_size, seq_q, seq_q)  # [batch_size, seq_len, seq_len]\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "      Implementation of the gelu activation function.\n",
    "      For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "      0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "      Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # [seq_len] -> [batch_size, seq_len]\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, seq_len, seq_len]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size, seq_len, d_model], k: [batch_size, seq_len, d_model], v: [batch_size, seq_len, d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size, n_heads, seq_len, d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size, n_heads, seq_len, d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size, n_heads, seq_len, d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # context: [batch_size, n_heads, seq_len, d_v], attn: [batch_size, n_heads, seq_len, seq_len]\n",
    "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size, seq_len, n_heads, d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual) # output: [batch_size, seq_len, d_model]\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, seq_len, d_model]\n",
    "        return enc_outputs\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        # fc2 is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.fc2.weight = embed_weight\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids) # [bach_size, seq_len, d_model]\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids) # [batch_size, maxlen, maxlen]\n",
    "        for layer in self.layers:\n",
    "            # output: [batch_size, max_len, d_model]\n",
    "            output = layer(output, enc_self_attn_mask)\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.fc(output[:, 0]) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2] predict isNext\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.activ2(self.linear(h_masked)) # [batch_size, max_pred, d_model]\n",
    "        logits_lm = self.fc2(h_masked) # [batch_size, max_pred, vocab_size]\n",
    "        return logits_lm, logits_clsf\n",
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n",
    "      logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "      loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\n",
    "      loss_lm = (loss_lm.float()).mean()\n",
    "      loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "      loss = loss_lm + loss_clsf\n",
    "      if (epoch + 1) % 10 == 0:\n",
    "          print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74652f27",
   "metadata": {},
   "source": [
    "**<font color = blue size=5 face=雅黑>Encoder</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code=[]\n",
    "for i, row in df_train1.iterrows():\n",
    "    a=row['code']\n",
    "    code.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7525ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderBERT, self).__init__()\n",
    "        bert = BERT()\n",
    "        newmodel = list(bert.children())[:-1]\n",
    "        self.bert = nn.Sequential(*newmodel)\n",
    "        # TO COMPLETE\n",
    "        # keep all layers of the pretrained net except the last one\n",
    "\n",
    "        \n",
    "    def forward(self, code):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "\n",
    "        # TO COMPLETE\n",
    "        # remember no gradients are needed\n",
    "        # return features \n",
    "        features = self.bert(code)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a386e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = EncoderBERT().to(device)\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    code,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b50dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648699fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=dict()\n",
    "with torch.no_grad():\n",
    "  for data in loader:\n",
    "    codes, labels = data\n",
    "    feature = encoder(codes)\n",
    "    feature1=feature.squeeze(3).squeeze(2)\n",
    "    features[labels[0]]=feature1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ee028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len], \\\n",
    "        requires_grad=False).cuda()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca03965",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "input_seq = batch.English.transpose(0,1)\n",
    "input_pad = EN_TEXT.vocab.stoi['<pad>']\n",
    "# creates mask with 0s wherever there is padding in the input\n",
    "input_msk = (input_seq != input_pad).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39bef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask as before\n",
    "target_seq = batch.French.transpose(0,1)\n",
    "target_pad = FR_TEXT.vocab.stoi['<pad>']\n",
    "target_msk = (target_seq != target_pad).unsqueeze(1)\n",
    "size = target_seq.size(1) # get seq_len for matrix\n",
    "nopeak_mask = np.triu(np.ones(1, size, size),\n",
    "k=1).astype('uint8')\n",
    "nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
    "target_msk = target_msk & nopeak_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "# calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de32a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
